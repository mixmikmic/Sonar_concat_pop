# # Scientific Computing in Python Workshop - Setup
# 

# Welcome to the workshop! We will begin by installing the required packages. Jupyter allows us to access the console with the ! symbol at the beginning of a cell. Take some time to look around the interface and aquaint yourself with Jupyter. 
# 
# **Note:** This installs requirements in the root conda environment. If you already have python installed before installing Anaconda Navigator, there may be some issues.
# 

# ## Step 1: Install Requirements
# This step assumes you have launched the Jupyter Notebook sever from your Anaconda Navigator dashboard and have installed the Python 3.6 version. Click on the cell below and press ```shift+enter```. It will take some time. 
# 

get_ipython().system('python -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose scikit-learn scikit-image tensorflow')


# ## Step 2: Verify the requirments. 
# Make sure everything is installed by pressing ```shift+enter``` on the cell below. It should quickly tell you that all requirements are satisfied.  
# 

get_ipython().system('python -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose scikit-learn scikit-image tensorflow')


# ## Step 3: Enjoy!
# Enjoy your journey through scientific computing in python. Click on the Jupyter icon at the top of the page and navigate to the ```WorkshopScipy/Notebooks``` folder and explore the notebooks. 
# 

# # Linear Regression
# ## The Solution Space
# ** November 2017 **
# 
# ** Andrew Riberio @ [AndrewRib.com](http://www.andrewrib.com) **
# 
# In this notebook we will explore the solution space of linear regression with the mean squared error. 
# 
# ** Note: ** This notebook contains interactive elements and certain latex snippets that will not render in github markdown. You must run this notebook on your local Jupyter notebook environment. 
# 
# ## Libraries
# 

import sympy as sp
import numpy as np
import matplotlib.pyplot as plt
import sklearn as sk
import sklearn.datasets as skd
from IPython.display import display
from sympy import MatrixSymbol, Matrix
from numpy.linalg import inv
from ipywidgets import interact, interactive, fixed, interact_manual
from mpl_toolkits.mplot3d import Axes3D

sp.init_printing(order='rev-lex',use_latex='mathjax')


# ## Linear Regression via Linear Least Squares
# 
# Let's begin with an interactive plot that will allow you to explore the relationship between:
# * **The Regression Line**
#     * Slope: The angle of the line. 
#     * Y-Intercept: Where the line intercepts the y axis. 
#     * Predicted Value = Slope * Dependent Variable + Y-Intercept
#     
#     
# * **Dataset**
#     * Generating function: Our dataset is synthetically generated by sampling a linear function, then adding noise. 
#     * Noise: a certain ammount of noise will be added to each sampling of the generating function. 
# 
# 
# * **Mean Squared Error**
#     * How well our regression line fits the data. The smaller the value, the better the fit. See the definition of the calculation below. In machine learning terms, this is our cost function. 
# 

def mse(yPredict,yActual):
    return np.square(yPredict.T-yActual.T).mean()
    
def h(slope,y_intercept):
    return lambda x:slope*x+y_intercept

def interactiveLine(slope,y_intercept,noise=50):
    X,Y = skd.make_regression(100,1,random_state=0, noise=noise)
    
    ys = np.apply_along_axis(h(slope,y_intercept), 0, X)
    
    plt.figure(figsize=(10,10))
    
    plt.title("Mean Squared Error: {0}".format(mse(ys,Y)),fontsize=15)
    plt.ylabel("Dependent Variable (Y)",fontsize=15)
    plt.xlabel("Independent Variable (X)",fontsize=15)
    plt.scatter(X,Y)
    plt.plot(X,ys)

    plt.show()
    

interact(interactiveLine, slope=(-150,150),y_intercept=(-100,100),noise=(0,150));


# ### Plotting the solution space.
# We will now enumerate all linear functions with all combinations of integer slopes and y-intercepts within [-150,150]. We therefore have:
# 
# * The X Axis:    150*2=300 unique slopes
# * The Y Axis:    150*2=300 unique y-intercepts
# * The X*Y Plane: 300^2 = 9,0000 unique linear regression lines. 
# 
# The $slope * yIntercept$ space creates a plane, the x and y axes, where each point represents a particular regression line. 
# 
# The z axis will represent the MSE for that regression line on the data:
# 

# $$
# \large MSE(\mathbf{x},\mathbf{y},h_\theta) = \frac{1}{m} \sum_{i=1}^m{(h_\theta(x^i) - y^i})^2  \tag{1} \\ 
# $$
# 
# Where:
# * $\theta_0$ = y_intercept 
# * $\theta_1$ = slope
# * $h_\theta(\mathbf{x})= \mathbf{\theta_{0}}+ \mathbf{\theta_{1}}\mathbf{x}$
# 
# In this section we will use the following synthetic dataset.
# 

X,Y = skd.make_regression(100,1,random_state=0, noise=50)

plt.scatter(X,Y)
plt.show()


# This can be done much more efficiently with matrix algebra, but this is the clearest way of doing this. 
# Enumerate over the entire space of linear functions within the ranges. 
# This cell will take a while to compute. 

# Uses h(slope,yInter)
def enumFnsAndMSE(possibleSlopes,possibleYInter,X_Data,Y_Data,scaleFactor=1):
    errorOverLines = []

    for slope in possibleSlopes:
        row = []

        for yInter in possibleYInter:
            lFN = h(slope,yInter)
            regressionYs = np.apply_along_axis(lFN, 0, X_Data)
            row.append( mse(regressionYs,Y_Data)/scaleFactor )

        errorOverLines.append(row)
        
    return errorOverLines

possibleSlopes = range(-150,150)
possibleYInter = range(-150,150)

errorOverLines = enumFnsAndMSE(possibleSlopes,possibleYInter,X,Y,600)


# Plot figure 
xx, yy = np.mgrid[-150:150, -150:150]

fig = plt.figure(figsize=(14,14))
ax = fig.gca(projection='3d',facecolor='gray')

ax.set_xlabel('Slope',fontSize=16)
ax.set_ylabel('Y-Intercept',fontSize=16)
ax.set_zlabel('Mean Squared Error',fontSize=16)

ax.plot_surface(xx, yy, errorOverLines, cmap=plt.cm.Reds, linewidth=0.2)
plt.show()


# The plot you see above is characteristic of convex optimization problems. If we put a ball anywhere on this surface, it will roll down to the bottom AKA the global minimum. A ball with no force applied will follow the steepest path -- the path where the gradient is largest. If we did apply some sideways force, it will roll across the sides until it settles to the bottom -- it may jump out of the bottom a few times untill it loses all of its kenetic energy. This method of using [momentum is actually usefull in some non-convex optimization problems, because it forces us to explore a wider area](https://distill.pub/2017/momentum/).
# 
# I'd like to make something clear that wasn't made so clear to me when I was first learning about convex optimization. Why do we care about things like a ball rolling down the slopes? The figure you see above is a plot of all possible solutions within some precision ( i.e. we had integer ranges for our possible slopes and y-intercepts, we could have had more precise floating point ranges ). Because this is a toy problem on a toy dataset, enumerating all possible solutions doesn't take too much time. When we start working with real world data with millions of datapoints, and much higher dimensions (i.e. polynomial regression) enumerating the entire solution space becomes combinatorially explosive. Therefore, in the absense of a closed-form solution, we are concerned with starting at some point, some solution, and making it better until we arrive at a solution we can't make better. Focusing on making a solution better and better until we can't is computationally cheap, expecially in the case of convex optimizaiton. 
# 
# In order to formalize this notion of the motion of a ball rolling around this space of solutions, we must introduce the notion of the gradient. We will explore this in the gradient descent notebook. 
# 

# ### Producing Optimal Linear Regression Lines
# There are two primary optimization methods to produce a line that minimizes the MSE (as explored above): 
# #### 1. Itterative Optimizaiton Methods
# Itterative methods behave simmilarly to how you adjusted to the parameters above to get the best fitting line: we adjust the slope and bias so that it keeps getting smaller. Because linear regression is a convex optimization problem, we are always garenteed that if we follow this gradient, the value will always get smaller until it reaches it's minimum value -- which is garenteed to be a global minimum in the case of convex optimization. 
# 
# Examples:
#     * Gradient Descent
#     * Newton's Method
#     * Exhaustive Search
# #### 2. Closed-Form Methods
# In some simpler optimization methods, as in linear least squares, we can actually derive the optimal solution analytically -- where we solve an equation and get the optimal result. 
# 
# Examples:
#     * Normal Equations
#     
# ** We will explore Gradient Descent and the Normal Equations in other notebooks. **
# 

# ### Example - Exhaustive Search
# When plotting the solition space above, we performed an exhaustive search within some precision -- being the intervals of the candidate slopes and y-intercepts. Let's use this and plot which line best fit our data in the exhaustive search. 
# 

def findMinimum(eol):
    rowLen = len(eol)
    colLen = len(eol[0])
    
    minVal = 100000000
    minCord = []
    
    for r_i in range(rowLen):
        for c_i in range(colLen):
            if(eol[r_i][c_i] < minVal):
                minVal = eol[r_i][c_i] 
                # We split the range in half to account for the negative values. 
                minCord = [r_i-rowLen/2,c_i-colLen/2]
                
    return [minCord,minVal]
            

bestLine = findMinimum(errorOverLines)
minSlope,minIntercept = findMinimum(errorOverLines)[0]
ys = np.apply_along_axis(h(minSlope,minIntercept), 0, X)

plt.scatter(X,Y)
plt.plot(X,ys)

plt.show()
print("MSE of line: {0}".format(bestLine[1]))


# # Exploring MNIST Manifolds
# ** November 2017 **
# 
# ** Andrew Riberio @ [AndrewRib.com](http://www.andrewrib.com) **
# 
# Pg 158 of the [Deep Learning Book](http://www.deeplearningbook.org/), "In the case of images, we can certainly think of many possible transformations that allow us to trace out a manifold in image space: we can gradually dim or brighten the lights, gradually move or rotate objects in the image, gradually alter the colors on the surfaces of objects, and so forth. Multiple manifolds are likely involved in most applications." 
# 
# In this notebook we will explore manifolds in the MNIST dataset by modeling the transformations which represent their traversal. Which manifolds describe the variation in the data? Are some transformations over representend? I.e. do we have a manifold bias? How would we quantify this? We wish to explore these questions here. 
# 
# Resources
# * [Visualizing MNIST: An Exploration of Dimensionality Reduction](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)
# * [A Beginner’s Guide to Eigenvectors, PCA, Covariance and Entropy](https://deeplearning4j.org/eigenvector)
# * [PCA Tutorial](https://strata.uga.edu/software/pdf/pcaTutorial.pdf)
# 
# ** Note: ** This notebook contains interactive elements and certain latex snippets that will not render in github markdown. 
# You must run this notebook on your local Jupyter notebook environment for interactive elements or render or if you wish to render just the latex by using the url of this repo with the online NBViewer. 
# 
# ## Libraries
# 

# Visualization 
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

#Interactive Components
from ipywidgets import interact

# Dataset Operations and Linear Algebra 
import pandas as pd
import numpy as np
import math
from scipy import stats

# Machine Learning
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# MNIST Dataset
from tensorflow.examples.tutorials.mnist import input_data


# ## Gathering data from files into a dictionary numpy arrays. 
# 

mnist = input_data.read_data_sets("MNIST_data/", one_hot=False)

unique, counts    = np.unique(mnist.train.labels, return_counts=True)
sortedCount       = sorted(dict(zip(unique, counts)).items(), key=lambda x: x[1],reverse=True)
sortedCountLabels = [i[0] for i in sortedCount]
sortedCountFreq   = [i[1] for i in sortedCount]

# TODO: Make more efficient.
# First we will zip the training labels with the training images
dataWithLabels = zip(mnist.train.labels, mnist.train.images)

# Now let's turn this into a dictionary where subsets of the images in respect
# to digit class are stored via the corresponding key.

# Init dataDict with keys [0,9] and empty lists.
digitDict = {}
for i in range(0,10):
    digitDict[i] = []

# Assign a list of image vectors to each corresponding digit class index.
for i in dataWithLabels:
    digitDict[i[0]].append(i[1])

# Convert the lists into numpy matricies. (could be done above, but I claim ignorace)
for i in range(0,10):
    digitDict[i] = np.matrix(digitDict[i])
    print("Digit {0} matrix shape: {1}".format(i,digitDict[i].shape))


#nImgs = digitDict[9].shape[0]
#avgImg = np.dot(digitDict[9].T, np.ones((nImgs,1)))/nImgs


# ## Approach 1: PCA
# 
# With principle components analysis, we can explore sources of variability in the MNIST dataset. The aim is to use this notion of variablity to describe the transformations found in the data. Once we can model the transformations found in the data, we can use them to describe various manifolds in MNIST. 
# 
# Let's begin by using this interactive section to explore the first two principle components as vectors plotted over each digit with respect to each digit class. 
# 

def pcaVectOnIMG(dataset,elmIndex):
    X_r = PCA(n_components=2).fit(dataset)
    pcaVect = X_r.transform(dataset[elmIndex])
    origin = [[14], [14]]
    plt.figure(figsize=(5,5))
    plt.imshow(dataset[elmIndex].reshape(28,28),cmap='gray')
    plt.quiver(*origin, pcaVect[:,0], pcaVect[:,1], color=['r','b','g'], scale=10)
    plt.show()
    
z =  lambda elmIndex=0,digit=1 :pcaVectOnIMG(digitDict[digit],elmIndex)

# Will error for some values of elmIndex.
interact( z, elmIndex=[0,8000],digit=[0,9])


# I hope you noticed how the first two principle components as a vector did a good job showing the left or right lean bias of the 1's; however, when a 1 is stretched or shrunk in the virtical direction, variablility not captured by the first two principle components essentially add noise and reduce the signifigance of where the vector is pointing. Here's an example where the vector does a great job at showing the variablity. 
# 

pcaVectOnIMG(digitDict[1],1340)


# Here's an example where our principle component vector does not do a good job at showing the variability. 
# 

pcaVectOnIMG(digitDict[1],2313)


# ### Experiment 1.1: Plotting the space of PCA Vectors
# The following interactive section is designed to explore the space different principle component vectors. Explore how different principle component vectors in R2 describes the variability in the data.
# 

def plotPCAVectors(data,componentIndexVec=[0,1],nComponents=120,filterDensity=50):
    n = data.shape[0]
    meanDigit = np.dot(data.T, np.ones((n,1)))/n
    
    data = data[0::filterDensity]
    X_r = PCA(n_components=nComponents).fit(data).transform(data)
    
    print("fIndex: the first principle component in the vector")
    print("sIndex: the second principle component in the vector")
    print("digit:  which digit class we are exploring.")
    
    plt.figure(figsize=(6,6))
    ax = plt.gca()
    
    origin = [[14], [14]] # origin point
    
    plt.imshow(meanDigit.reshape(28,28),cmap='gray')
    plt.quiver(*origin, X_r[:,componentIndexVec[0]], X_r[:,componentIndexVec[1]], color=['r','b','g'], scale=13)

    plt.show()
    
z = lambda fIndex=0,sIndex=1,digit=1:plotPCAVectors(digitDict[digit],[fIndex,sIndex])
    

interact(z,fIndex=[0,119],sIndex=[0,119],digit=[0,9])


# To obtain vectors which best show the variability in R2, we would need to find particular principle component vectors and combine them with vector mathematics. I will take a side step to explore what the magnitude of these vectors mean for the underlying images. 
# 
# ### Experiment 1.2: Magnitude of PCA Vectors
# 

def pcaR2Vects(dataset):
    return PCA(n_components=2).fit(dataset).transform(dataset)

#pcaR2Vects(digitDict[1])

def dataWithPCA_R2Vects(dataset):
    pcaVects = pcaR2Vects(dataset)
    print(dataset.shape)
    print(pcaVects.shape)
    return zip(dataset,pcaVects)

def R2Norm(vects):
    return np.linalg.norm( pcaR2Vects(ds), 2 , axis= 1)

ds = digitDict[1]
vectMag = list(zip(list(range(0,ds.shape[0],1)),R2Norm( pcaR2Vects(ds) )))

rs = sorted(vectMag, key=lambda x: x[1])
nCases = len(rs)

def tst1(elm=0):
    plt.imshow(ds[rs[elm][0]].reshape(28,28),cmap='gray')
    plt.show()
    
interact(tst1,elm=[0,20])


interact(tst1,elm=[100,200])


# ## Approach 2: MNIST Statistics
# In this section we will do things like explore the co-variance matrix of the digits. 
# 

# ## Approach 3: K-Means Manifold Clustering
# Here we will work within digit classes. 
# 
# This first interactive section will allow you to visualize the mean pixel intensity across all samples within each digit class giving a visualization of the average digit. 
# 

def meanDigitClipped(digitClass):
    n = digitClass.shape[0]
    meanDigit = np.clip( np.dot(digitClass.T, np.ones((n,1)))/n, 0.00001,1)
    return meanDigit

def meanDigitVis(digitClass=0):
    meanImg = meanDigitClipped(digitDict[digitClass])
    plt.figure(figsize=(6,6))
    plt.imshow(meanImg.reshape(28,28),cmap='gray')
    plt.show()

interact(meanDigitVis,digitClass=[0,9])


# We will now define a helper function which will allow us to perform kMeans and return the results in a form easy to work with. 
# 

def KMeanDict(data,nClusters):
    kmLabels = KMeans(n_clusters=nClusters, random_state=None).fit(data).labels_
    classDict = {label: data[label==kmLabels] for label in np.unique(kmLabels)}    

    for i in classDict:
        classDict[i] = np.matrix(classDict[i])
        
    return classDict


# In this next interactive section you can explore different average digits across different partitions of the data. 
# 

def makeSubplots(nGridRow,nGridCol,figsize=(20,20)):
    sps = []
    fig = plt.figure(figsize=figsize)
    for i in range(1,(nGridRow*nGridCol)+1):
        sps.append(fig.add_subplot(nGridRow,nGridCol,i))
    return (fig,sps)

def kVis(digit=1,nClasses=4):
    figRows,figCols = (math.ceil(nClasses/4),4)
    fig,sps = makeSubplots(figRows,figCols)
    
    kDict = KMeanDict(digitDict[digit],nClasses)
    
    for i in kDict:
        md = meanDigitClipped(kDict[i])
        sps[i].imshow(md.reshape(28,28),cmap='gray')
  
        

    plt.show()
   
interact(kVis,digit=[0,9],nClasses=[2,10])





