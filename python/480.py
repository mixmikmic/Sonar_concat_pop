# **Source of the materials**: Biopython cookbook (adapted)
# <font color='red'>Status: Draft</font>
# 

# # About the contents
# 

# The contents of these notebooks have several sources (mostly the Biopython Tutorial and Peter Cock's workshop). Credit to the source of will be given at the start of the notebook.
# 
# The current version of these notebooks losely follows the structure of the Tutorial.
# 
# As IPython Notebook is generaly available with matplotlib, NumPy and SciPy I will make use of these if I found it relevant.
# 

# ## Authorship
# 

# The authors of the Biopython tutorial and cookbook are: Jeff Chang, Brad Chapman, Iddo Friedberg, Thomas Hamelryck,
# Michiel de Hoon, Peter Cock, Tiago Antao, Eric Talevich, Bartek Wilczynski
# 

# The people doing the conversion to IPython Notebook format are: Vincent Davis and Tiago Antao
# 

# KEGG
# ====
# 
# KEGG (<http://www.kegg.jp/>) is a database resource for understanding
# high-level functions and utilities of the biological system, such as the
# cell, the organism and the ecosystem, from molecular-level information,
# especially large-scale molecular datasets generated by genome sequencing
# and other high-throughput experimental technologies.
# 
# Please note that the KEGG parser implementation in Biopython is
# incomplete. While the KEGG website indicates many flat file formats,
# only parsers and writers for compound, enzyme, and map are currently
# implemented. However, a generic parser is implemented to handle the
# other formats.
# 
# Parsing KEGG records
# --------------------
# 
# Parsing a KEGG record is as simple as using any other file format parser
# in Biopython. (Before running the following codes, please open
# http://rest.kegg.jp/get/ec:5.4.2.2 with your web browser and save it as
# ec\_5.4.2.2.txt.)
# 

get_ipython().system('wget http://rest.kegg.jp/get/ec:5.4.2.2 -O ec_5.4.2.2.txt')


from Bio.KEGG import Enzyme

records = Enzyme.parse(open("ec_5.4.2.2.txt"))

record = list(records)[0]

record.classname


record.entry


# The following section will shows how to download the above enzyme using
# the KEGG api as well as how to use the generic parser with data that
# does not have a custom parser implemented.
# 
# Querying the KEGG API
# ---------------------
# 
# Biopython has full support for the querying of the KEGG api. Querying
# all KEGG endpoints are supported; all methods documented by KEGG
# (<http://www.kegg.jp/kegg/rest/keggapi.html>) are supported. The
# interface has some validation of queries which follow rules defined on
# the KEGG site. However, invalid queries which return a 400 or 404 must
# be handled by the user.
# 
# First, here is how to extend the above example by downloading the
# relevant enzyme and passing it through the Enzyme parser.
# 

from Bio.KEGG import REST

from Bio.KEGG import Enzyme

request = REST.kegg_get("ec:5.4.2.2")

open("ec_5.4.2.2.txt", 'w').write(request.read().decode("utf-8"))


records = Enzyme.parse(open("ec_5.4.2.2.txt"))

record = list(records)[0]

record.classname


record.entry


# Now, here’s a more realistic example which shows a combination of
# querying the KEGG API. This will demonstrate how to extract a unique set
# of all human pathway gene symbols which relate to DNA repair. The steps
# that need to be taken to do so are as follows. First, we need to get a
# list of all human pathways. Secondly, we need to filter those for ones
# which relate to “repair”. Lastly, we need to get a list of all the gene
# symbols in all repair pathways.
# 

from Bio.KEGG import REST

human_pathways = REST.kegg_list("pathway", "hsa").read()

human_pathways.decode("utf-8").split("\n")[0:5]


# Filter all human pathways for repair pathways
repair_pathways = []
for line in human_pathways.decode("utf-8").rstrip().split("\n"):
    entry, description = line.split("\t")
    if "repair" in description:
        repair_pathways.append(entry)

repair_pathways


# Get the genes for pathways and add them to a list
repair_genes = [] 
for pathway in repair_pathways:
    pathway_file = REST.kegg_get(pathway).read()  # query and read each pathway

    # iterate through each KEGG pathway file, keeping track of which section
    # of the file we're in, only read the gene in each pathway
    current_section = None
    for line in pathway_file.decode("utf-8").rstrip().split("\n"):
        section = line[:12].strip()  # section names are within 12 columns
        if not section == "":
            current_section = section
        
        if current_section == "GENE":
            gene_identifiers, gene_description = line[12:].split("; ")
            gene_id, gene_symbol = gene_identifiers.split()

            if not gene_symbol in repair_genes:
                repair_genes.append(gene_symbol)

print("There are %d repair pathways and %d repair genes. The genes are:" %         (len(repair_pathways), len(repair_genes)))
print(", ".join(repair_genes))


# The KEGG API wrapper is compatible with all endpoints. Usage is
# essentially replacing all slashes in the url with commas and using that
# list as arguments to the corresponding method in the KEGG module. Here
# are a few examples from the api documentation
# (<http://www.kegg.jp/kegg/docs/keggapi.html>).
# 
#     list(/hsa:10458+ece:Z5100, ->, REST.kegg_list(["hsa:10458",, "ece:Z5100"]))
#     find(/compound/300-310/mol_weight, ->, REST.kegg_find("compound",, "300-310",, "mol_weight"))
#     get(/hsa:10458+ece:Z5100/aaseq, ->, REST.kegg_get(["hsa:10458",, "ece:Z5100"],, "aaseq"))
# 




# **Source of the materials**: Biopython cookbook (adapted)
# <font color='red'>Status: Draft</font>

# Supervised learning methods
# ===========================
# 
# Note the supervised learning methods described in this chapter all
# require Numerical Python (numpy) to be installed.
# 
# The Logistic Regression Model {#sec:LogisticRegression}
# -----------------------------
# 
# ### Background and Purpose
# 
# Logistic regression is a supervised learning approach that attempts to
# distinguish $K$ classes from each other using a weighted sum of some
# predictor variables $x_i$. The logistic regression model is used to
# calculate the weights $\beta_i$ of the predictor variables. In
# Biopython, the logistic regression model is currently implemented for
# two classes only ($K = 2$); the number of predictor variables has no
# predefined limit.
# 
# As an example, let’s try to predict the operon structure in bacteria. An
# operon is a set of adjacent genes on the same strand of DNA that are
# transcribed into a single mRNA molecule. Translation of the single mRNA
# molecule then yields the individual proteins. For <span>*Bacillus
# subtilis*</span>, whose data we will be using, the average number of
# genes in an operon is about 2.4.
# 
# As a first step in understanding gene regulation in bacteria, we need to
# know the operon structure. For about 10% of the genes in <span>*Bacillus
# subtilis*</span>, the operon structure is known from experiments. A
# supervised learning method can be used to predict the operon structure
# for the remaining 90% of the genes.
# 
# For such a supervised learning approach, we need to choose some
# predictor variables $x_i$ that can be measured easily and are somehow
# related to the operon structure. One predictor variable might be the
# distance in base pairs between genes. Adjacent genes belonging to the
# same operon tend to be separated by a relatively short distance, whereas
# adjacent genes in different operons tend to have a larger space between
# them to allow for promoter and terminator sequences. Another predictor
# variable is based on gene expression measurements. By definition, genes
# belonging to the same operon have equal gene expression profiles, while
# genes in different operons are expected to have different expression
# profiles. In practice, the measured expression profiles of genes in the
# same operon are not quite identical due to the presence of measurement
# errors. To assess the similarity in the gene expression profiles, we
# assume that the measurement errors follow a normal distribution and
# calculate the corresponding log-likelihood score.
# 
# We now have two predictor variables that we can use to predict if two
# adjacent genes on the same strand of DNA belong to the same operon:
# 
# -   $x_1$: the number of base pairs between them;
# 
# -   $x_2$: their similarity in expression profile.
# 
# In a logistic regression model, we use a weighted sum of these two
# predictors to calculate a joint score $S$:
# $$S = \beta_0 + \beta_1 x_1 + \beta_2 x_2.$$ The logistic regression
# model gives us appropriate values for the parameters $\beta_0$,
# $\beta_1$, $\beta_2$ using two sets of example genes:
# 
# -   OP: Adjacent genes, on the same strand of DNA, known to belong to
#     the same operon;
# 
# -   NOP: Adjacent genes, on the same strand of DNA, known to belong to
#     different operons.
# 
# In the logistic regression model, the probability of belonging to a
# class depends on the score via the logistic function. For the two
# classes OP and NOP, we can write this as $$\begin{aligned}
# \Pr(\mathrm{OP}|x_1, x_2) & = & \frac{\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}{1+\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} \label{eq:OP} \\
# \Pr(\mathrm{NOP}|x_1, x_2) & = & \frac{1}{1+\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} \label{eq:NOP}\end{aligned}$$
# Using a set of gene pairs for which it is known whether they belong to
# the same operon (class OP) or to different operons (class NOP), we can
# calculate the weights $\beta_0$, $\beta_1$, $\beta_2$ by maximizing the
# log-likelihood corresponding to the probability functions (\[eq:OP\])
# and (\[eq:NOP\]).
# 
# ### Training the logistic regression model {#subsec:LogisticRegressionTraining}
# 
#                     Gene pair                    Intergene distance ($x_1$)   Gene expression score ($x_2$)   Class
#   --------------------------------------------- ---------------------------- ------------------------------- -------
#    <span>*cotJA*</span> — <span>*cotJB*</span>              -53                          -200.78               OP
#     <span>*yesK*</span> — <span>*yesL*</span>               117                          -267.14               OP
#     <span>*lplA*</span> — <span>*lplB*</span>                57                          -163.47               OP
#     <span>*lplB*</span> — <span>*lplC*</span>                16                          -190.30               OP
#     <span>*lplC*</span> — <span>*lplD*</span>                11                          -220.94               OP
#     <span>*lplD*</span> — <span>*yetF*</span>                85                          -193.94               OP
#     <span>*yfmT*</span> — <span>*yfmS*</span>                16                          -182.71               OP
#     <span>*yfmF*</span> — <span>*yfmE*</span>                15                          -180.41               OP
#     <span>*citS*</span> — <span>*citT*</span>               -26                          -181.73               OP
#     <span>*citM*</span> — <span>*yflN*</span>                58                          -259.87               OP
#     <span>*yfiI*</span> — <span>*yfiJ*</span>               126                          -414.53               NOP
#     <span>*lipB*</span> — <span>*yfiQ*</span>               191                          -249.57               NOP
#     <span>*yfiU*</span> — <span>*yfiV*</span>               113                          -265.28               NOP
#     <span>*yfhH*</span> — <span>*yfhI*</span>               145                          -312.99               NOP
#     <span>*cotY*</span> — <span>*cotX*</span>               154                          -213.83               NOP
#     <span>*yjoB*</span> — <span>*rapA*</span>               147                          -380.85               NOP
#     <span>*ptsI*</span> — <span>*splA*</span>                93                          -291.13               NOP
# 
#   : Adjacent gene pairs known to belong to the same operon (class OP) or
#   to different operons (class NOP). Intergene distances are negative if
#   the two genes overlap.
# 
# \[table:training\]
# 
# Table \[table:training\] lists some of the <span>*Bacillus
# subtilis*</span> gene pairs for which the operon structure is known.
# Let’s calculate the logistic regression model from these data:
# 
# 

from Bio import LogisticRegression
xs = [[-53, -200.78], [117, -267.14], [57, -163.47], [16, -190.30],
      [11, -220.94], [85, -193.94], [16, -182.71], [15, -180.41],
      [-26, -181.73], [58, -259.87], [126, -414.53], [191, -249.57],
      [113, -265.28], [145, -312.99], [154, -213.83], [147, -380.85],[93, -291.13]]
ys = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
model = LogisticRegression.train(xs, ys)


# 
# Here, `xs` and `ys` are the training data: `xs` contains the predictor
# variables for each gene pair, and `ys` specifies if the gene pair
# belongs to the same operon (`1`, class OP) or different operons (`0`,
# class NOP). The resulting logistic regression model is stored in
# `model`, which contains the weights $\beta_0$, $\beta_1$, and $\beta_2$:
# 
# 

model.beta


# 
# Note that $\beta_1$ is negative, as gene pairs with a shorter intergene
# distance have a higher probability of belonging to the same operon
# (class OP). On the other hand, $\beta_2$ is positive, as gene pairs
# belonging to the same operon typically have a higher similarity score of
# their gene expression profiles. The parameter $\beta_0$ is positive due
# to the higher prevalence of operon gene pairs than non-operon gene pairs
# in the training data.
# 
# The function `train` has two optional arguments: `update_fn` and
# `typecode`. The `update_fn` can be used to specify a callback function,
# taking as arguments the iteration number and the log-likelihood. With
# the callback function, we can for example track the progress of the
# model calculation (which uses a Newton-Raphson iteration to maximize the
# log-likelihood function of the logistic regression model):
# 
# 

def show_progress(iteration, loglikelihood):
    print("Iteration:", iteration, "Log-likelihood function:", loglikelihood)

model = LogisticRegression.train(xs, ys, update_fn=show_progress)


# 
# The iteration stops once the increase in the log-likelihood function is
# less than 0.01. If no convergence is reached after 500 iterations, the
# `train` function returns with an `AssertionError`.
# 
# The optional keyword `typecode` can almost always be ignored. This
# keyword allows the user to choose the type of Numeric matrix to use. In
# particular, to avoid memory problems for very large problems, it may be
# necessary to use single-precision floats (Float8, Float16, etc.) rather
# than double, which is used by default.
# 
# ### Using the logistic regression model for classification
# 
# Classification is performed by calling the `classify` function. Given a
# logistic regression model and the values for $x_1$ and $x_2$ (e.g. for a
# gene pair of unknown operon structure), the `classify` function returns
# `1` or `0`, corresponding to class OP and class NOP, respectively. For
# example, let’s consider the gene pairs <span>*yxcE*</span>,
# <span>*yxcD*</span> and <span>*yxiB*</span>, <span>*yxiA*</span>:
# 
#                    Gene pair                   Intergene distance $x_1$   Gene expression score $x_2$
#   ------------------------------------------- -------------------------- -----------------------------
#    <span>*yxcE*</span> — <span>*yxcD*</span>              6                     -173.143442352
#    <span>*yxiB*</span> — <span>*yxiA*</span>             309                    -271.005880394
# 
#   : Adjacent gene pairs of unknown operon status.
# 
# The logistic regression model classifies <span>*yxcE*</span>,
# <span>*yxcD*</span> as belonging to the same operon (class OP), while
# <span>*yxiB*</span>, <span>*yxiA*</span> are predicted to belong to
# different operons:
# 
# 

print("yxcE, yxcD:", LogisticRegression.classify(model, [6, -173.143442352]))
print("yxiB, yxiA:", LogisticRegression.classify(model, [309, -271.005880394]))


# 
# (which, by the way, agrees with the biological literature).
# 
# To find out how confident we can be in these predictions, we can call
# the `calculate` function to obtain the probabilities (equations
# (\[eq:OP\]) and \[eq:NOP\]) for class OP and NOP. For
# <span>*yxcE*</span>, <span>*yxcD*</span> we find
# 
# 

q, p = LogisticRegression.calculate(model, [6, -173.143442352])
print("class OP: probability =", p, "class NOP: probability =", q)


# 
# and for <span>*yxiB*</span>, <span>*yxiA*</span>
# 
# 

q, p = LogisticRegression.calculate(model, [309, -271.005880394])
print("class OP: probability =", p, "class NOP: probability =", q)


# 
# To get some idea of the prediction accuracy of the logistic regression
# model, we can apply it to the training data:
# 
# 

for i in range(len(ys)):
    print("True:", ys[i], "Predicted:", LogisticRegression.classify(model, xs[i]))


# 
# showing that the prediction is correct for all but one of the gene
# pairs. A more reliable estimate of the prediction accuracy can be found
# from a leave-one-out analysis, in which the model is recalculated from
# the training data after removing the gene to be predicted:
# 
# 

for i in range(len(ys)):
    print("True:", ys[i], "Predicted:", LogisticRegression.classify(model, xs[i]))


# 
# The leave-one-out analysis shows that the prediction of the logistic
# regression model is incorrect for only two of the gene pairs, which
# corresponds to a prediction accuracy of 88%.
# 
# ### Logistic Regression, Linear Discriminant Analysis, and Support Vector Machines
# 
# The logistic regression model is similar to linear discriminant
# analysis. In linear discriminant analysis, the class probabilities also
# follow equations (\[eq:OP\]) and (\[eq:NOP\]). However, instead of
# estimating the coefficients $\beta$ directly, we first fit a normal
# distribution to the predictor variables $x$. The coefficients $\beta$
# are then calculated from the means and covariances of the normal
# distribution. If the distribution of $x$ is indeed normal, then we
# expect linear discriminant analysis to perform better than the logistic
# regression model. The logistic regression model, on the other hand, is
# more robust to deviations from normality.
# 
# Another similar approach is a support vector machine with a linear
# kernel. Such an SVM also uses a linear combination of the predictors,
# but estimates the coefficients $\beta$ from the predictor variables $x$
# near the boundary region between the classes. If the logistic regression
# model (equations (\[eq:OP\]) and (\[eq:NOP\])) is a good description for
# $x$ away from the boundary region, we expect the logistic regression
# model to perform better than an SVM with a linear kernel, as it relies
# on more data. If not, an SVM with a linear kernel may perform better.
# 
# Trevor Hastie, Robert Tibshirani, and Jerome Friedman: <span>*The
# Elements of Statistical Learning. Data Mining, Inference, and
# Prediction*</span>. Springer Series in Statistics, 2001. Chapter 4.4.
# 
# $k$-Nearest Neighbors
# ---------------------
# 
# ### Background and purpose
# 
# The $k$-nearest neighbors method is a supervised learning approach that
# does not need to fit a model to the data. Instead, data points are
# classified based on the categories of the $k$ nearest neighbors in the
# training data set.
# 
# In Biopython, the $k$-nearest neighbors method is available in
# `Bio.kNN`. To illustrate the use of the $k$-nearest neighbor method in
# Biopython, we will use the same operon data set as in section
# \[sec:LogisticRegression\].
# 
# ### Initializing a $k$-nearest neighbors model
# 
# Using the data in Table \[table:training\], we create and initialize a
# $k$-nearest neighbors model as follows:
# 
# 

from Bio import kNN
k = 3
model = kNN.train(xs, ys, k)


# 
# where `xs` and `ys` are the same as in Section
# \[subsec:LogisticRegressionTraining\]. Here, `k` is the number of
# neighbors $k$ that will be considered for the classification. For
# classification into two classes, choosing an odd number for $k$ lets you
# avoid tied votes. The function name `train` is a bit of a misnomer,
# since no model training is done: this function simply stores `xs`, `ys`,
# and `k` in `model`.
# 
# ### Using a $k$-nearest neighbors model for classification
# 
# To classify new data using the $k$-nearest neighbors model, we use the
# `classify` function. This function takes a data point $(x_1,x_2)$ and
# finds the $k$-nearest neighbors in the training data set `xs`. The data
# point $(x_1, x_2)$ is then classified based on which category (`ys`)
# occurs most among the $k$ neighbors.
# 
# For the example of the gene pairs <span>*yxcE*</span>,
# <span>*yxcD*</span> and <span>*yxiB*</span>, <span>*yxiA*</span>, we
# find:
# 
# 

x = [6, -173.143442352]
print("yxcE, yxcD:", kNN.classify(model, x))


x = [309, -271.005880394]
print("yxiB, yxiA:", kNN.classify(model, x))


# 
# In agreement with the logistic regression model, <span>*yxcE*</span>,
# <span>*yxcD*</span> are classified as belonging to the same operon
# (class OP), while <span>*yxiB*</span>, <span>*yxiA*</span> are predicted
# to belong to different operons.
# 
# The `classify` function lets us specify both a distance function and a
# weight function as optional arguments. The distance function affects
# which $k$ neighbors are chosen as the nearest neighbors, as these are
# defined as the neighbors with the smallest distance to the query point
# $(x, y)$. By default, the Euclidean distance is used. Instead, we could
# for example use the city-block (Manhattan) distance:
# 
# 

def cityblock(x1, x2):
    assert len(x1)==2
    assert len(x2)==2
    distance = abs(x1[0]-x2[0]) + abs(x1[1]-x2[1])
    return distance

x = [6, -173.143442352]
print("yxcE, yxcD:", kNN.classify(model, x, distance_fn = cityblock))


# 
# The weight function can be used for weighted voting. For example, we may
# want to give closer neighbors a higher weight than neighbors that are
# further away:
# 
# 

from math import exp
def weight(x1, x2):
    assert len(x1)==2
    assert len(x2)==2
    return exp(-abs(x1[0]-x2[0]) - abs(x1[1]-x2[1]))

x = [6, -173.143442352]
print("yxcE, yxcD:", kNN.classify(model, x, weight_fn = weight))


# 
# By default, all neighbors are given an equal weight.
# 
# To find out how confident we can be in these predictions, we can call
# the `calculate` function, which will calculate the total weight assigned
# to the classes OP and NOP. For the default weighting scheme, this
# reduces to the number of neighbors in each category. For
# <span>*yxcE*</span>, <span>*yxcD*</span>, we find
# 
# 

x = [6, -173.143442352]
weight = kNN.calculate(model, x)
print("class OP: weight =", weight[0], "class NOP: weight =", weight[1])


# 
# which means that all three neighbors of `x1`, `x2` are in the NOP class.
# As another example, for <span>*yesK*</span>, <span>*yesL*</span> we find
# 
# 

x = [117, -267.14]
weight = kNN.calculate(model, x)
print("class OP: weight =", weight[0], "class NOP: weight =", weight[1])


# 
# which means that two neighbors are operon pairs and one neighbor is a
# non-operon pair.
# 
# To get some idea of the prediction accuracy of the $k$-nearest neighbors
# approach, we can apply it to the training data:
# 
# 

for i in range(len(ys)):
    print("True:", ys[i], "Predicted:", kNN.classify(model, xs[i]))


# 
# showing that the prediction is correct for all but two of the gene
# pairs. A more reliable estimate of the prediction accuracy can be found
# from a leave-one-out analysis, in which the model is recalculated from
# the training data after removing the gene to be predicted:
# 
# 

for i in range(len(ys)):
    model = kNN.train(xs[:i]+xs[i+1:], ys[:i]+ys[i+1:], 3)
    print("True:", ys[i], "Predicted:", kNN.classify(model, xs[i]))





# **Source of the materials**: Biopython cookbook (adapted)
# <font color='red'>Status: Draft</font>
# 

# References
# ---------
# 
# 1. Peter J. A. Cock, Tiago Antao, Jeffrey T. Chang, Brad A. Chapman, Cymon J. Cox, Andrew Dalke, Iddo Friedberg, Thomas Hamelryck, Frank Kauff, Bartek Wilczynski, Michiel J. L. de Hoon: “Biopython: freely available Python tools for computational molecular biology and bioinformatics”. Bioinformatics 25 (11), 1422–1423 (2009). doi:10.1093/bioinformatics/btp163,
# 
# 2. Leighton Pritchard, Jennifer A. White, Paul R.J. Birch, Ian K. Toth: “GenomeDiagram: a python package for the visualization of large-scale genomic data”. Bioinformatics 22 (5): 616–617 (2006). doi:10.1093/bioinformatics/btk021,
# 
# 3. Ian K. Toth, Leighton Pritchard, Paul R. J. Birch: “Comparative genomics reveals what makes an enterobacterial plant pathogen”. Annual Review of Phytopathology 44: 305–336 (2006). doi:10.1146/annurev.phyto.44.070505.143444,
# 
# 4. Géraldine A. van der Auwera, Jaroslaw E. Król, Haruo Suzuki, Brian Foster, Rob van Houdt, Celeste J. Brown, Max Mergeay, Eva M. Top: “Plasmids captured in C. metallidurans CH34: defining the PromA family of broad-host-range plasmids”. Antonie van Leeuwenhoek 96 (2): 193–204 (2009). doi:10.1007/s10482-009-9316-9
# 
# 5. Caroline Proux, Douwe van Sinderen, Juan Suarez, Pilar Garcia, Victor Ladero, Gerald F. Fitzgerald, Frank Desiere, Harald Brüssow: “The dilemma of phage taxonomy illustrated by comparative genomics of Sfi21-Like Siphoviridae in lactic acid bacteria”. Journal of Bacteriology 184 (21): 6026–6036 (2002). http://dx.doi.org/10.1128/JB.184.21.6026-6036.2002
# 
# 6. Florian Jupe, Leighton Pritchard, Graham J. Etherington, Katrin MacKenzie, Peter JA Cock, Frank Wright, Sanjeev Kumar Sharma1, Dan Bolser, Glenn J Bryan, Jonathan DG Jones, Ingo Hein: “Identification and localisation of the NB-LRR gene family within the potato genome”. BMC Genomics 13: 75 (2012). http://dx.doi.org/10.1186/1471-2164-13-75
# 
# 7. Peter J. A. Cock, Christopher J. Fields, Naohisa Goto, Michael L. Heuer, Peter M. Rice: “The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants”. Nucleic Acids Research 38 (6): 1767–1771 (2010). doi:10.1093/nar/gkp1137
# 
# 8. Patrick O. Brown, David Botstein: “Exploring the new world of the genome with DNA microarrays”. Nature Genetics 21 (Supplement 1), 33–37 (1999). doi:10.1038/4462
# 
# 9. Eric Talevich, Brandon M. Invergo, Peter J.A. Cock, Brad A. Chapman: “Bio.Phylo: A unified toolkit for processing, analyzing and visualizing phylogenetic trees in Biopython”. BMC Bioinformatics 13: 209 (2012). doi:10.1186/1471-2105-13-209
# 
# 10. Athel Cornish-Bowden: “Nomenclature for incompletely specified bases in nucleic acid sequences: Recommendations 1984.” Nucleic Acids Research 13 (9): 3021–3030 (1985). doi:10.1093/nar/13.9.3021
# 
# 11. Douglas R. Cavener: “Comparison of the consensus sequence flanking translational start sites in Drosophila and vertebrates.” Nucleic Acids Research 15 (4): 1353–1361 (1987). doi:10.1093/nar/15.4.1353
# 
# 12. Timothy L. Bailey and Charles Elkan: “Fitting a mixture model by expectation maximization to discover motifs in biopolymers”, Proceedings of the Second International Conference on Intelligent Systems for Molecular Biology 28–36. AAAI Press, Menlo Park, California (1994).
# 
# 13. Brad Chapman and Jeff Chang: “Biopython: Python tools for computational biology”. ACM SIGBIO Newsletter 20 (2): 15–19 (August 2000).
# 
# 14. Michiel J. L. de Hoon, Seiya Imoto, John Nolan, Satoru Miyano: “Open source clustering software”. Bioinformatics 20 (9): 1453–1454 (2004). doi:10.1093/bioinformatics/bth078
# 
# 15. Michiel B. Eisen, Paul T. Spellman, Patrick O. Brown, David Botstein: “Cluster analysis and display of genome-wide expression patterns”. Proceedings of the National Academy of Science USA 95 (25): 14863–14868 (1998). doi:10.1073/pnas.96.19.10943-c
# 
# 16. Gene H. Golub, Christian Reinsch: “Singular value decomposition and least squares solutions”. In Handbook for Automatic Computation, 2, (Linear Algebra) (J. H. Wilkinson and C. Reinsch, eds), 134–151. New York: Springer-Verlag (1971).
# 
# 17. Gene H. Golub, Charles F. Van Loan: Matrix computations, 2nd edition (1989).
# 
# 18. Thomas Hamelryck and Bernard Manderick: 11PDB parser and structure class implemented in Python”. Bioinformatics, 19 (17): 2308–2310 (2003) doi: 10.1093/bioinformatics/btg299.
# 
# 19. Thomas Hamelryck: “Efficient identification of side-chain patterns using a multidimensional index tree”. Proteins 51 (1): 96–108 (2003). doi:10.1002/prot.10338
# 
# 20. Thomas Hamelryck: “An amino acid has two sides; A new 2D measure provides a different view of solvent exposure”. Proteins 59 (1): 29–48 (2005). doi:10.1002/prot.20379.
# 
# 21. John A. Hartiga. Clustering algorithms. New York: Wiley (1975).
# 
# 22. Anil L. Jain, Richard C. Dubes: Algorithms for clustering data. Englewood Cliffs, N.J.: Prentice Hall (1988).
# 
# 23. Voratas Kachitvichyanukul, Bruce W. Schmeiser: Binomial Random Variate Generation. Communications of the ACM 31 (2): 216–222 (1988). doi:10.1145/42372.42381
# 
# 24. Teuvo Kohonen: “Self-organizing maps”, 2nd Edition. Berlin; New York: Springer-Verlag (1997).
# 
# 25. Pierre L’Ecuyer: “Efficient and Portable Combined Random Number Generators.” Communications of the ACM 31 (6): 742–749,774 (1988). doi:10.1145/62959.62969
# 
# 26. Indraneel Majumdar, S. Sri Krishna, Nick V. Grishin: “PALSSE: A program to delineate linear secondary structural elements from protein structures.” BMC Bioinformatics, 6: 202 (2005). doi:10.1186/1471-2105-6-202.
# 
# 27. V. Matys, E. Fricke, R. Geffers, E. Gössling, M. Haubrock, R. Hehl, K. Hornischer, D. Karas, A.E. Kel, O.V. Kel-Margoulis, D.U. Kloos, S. Land, B. Lewicki-Potapov, H. Michael, R. Münch, I. Reuter, S. Rotert, H. Saxel, M. Scheer, S. Thiele, E. Wingender E: “TRANSFAC: transcriptional regulation, from patterns to profiles.” Nucleic Acids Research 31 (1): 374–378 (2003). doi:10.1093/nar/gkg108
# 
# 28. Robin Sibson: “SLINK: An optimally efficient algorithm for the single-link cluster method”. The Computer Journal 16 (1): 30–34 (1973). doi:10.1093/comjnl/16.1.30
# 
# 29. George W. Snedecor, William G. Cochran: Statistical methods. Ames, Iowa: Iowa State University Press (1989).
# 
# 30. Pablo Tamayo, Donna Slonim, Jill Mesirov, Qing Zhu, Sutisak Kitareewan, Ethan Dmitrovsky, Eric S. Lander, Todd R. Golub: “Interpreting patterns of gene expression with self-organizing maps: Methods and application to hematopoietic differentiation”. Proceedings of the National Academy of Science USA 96 (6): 2907–2912 (1999). doi:10.1073/pnas.96.6.2907
# 
# 31. Robert C. Tryon, Daniel E. Bailey: Cluster analysis. New York: McGraw-Hill (1970).
# 
# 32. John W. Tukey: “Exploratory data analysis”. Reading, Mass.: Addison-Wesley Pub. Co. (1977).
# 
# 33. Ka Yee Yeung, Walter L. Ruzzo: “Principal Component Analysis for clustering gene expression data”. Bioinformatics 17 (9): 763–774 (2001). doi:10.1093/bioinformatics/17.9.763
# 
# 34. Alok Saldanha: “Java Treeview—extensible visualization of microarray data”. Bioinformatics 20 (17): 3246–3248 (2004). http://dx.doi.org/10.1093/bioinformatics/bth349
# 




# **Source of the materials**: Biopython Tutorial and Cookbook (adapted)
# 

# <img src="images/biopython.jpg">
# 

# # Introduction
# 

# ## What is Biopython?
# 

# The Biopython Project is an international association of developers of freely available Python (http://www.python.org) tools for computational molecular biology. Python is an object oriented, interpreted, flexible language that is becoming increasingly popular for scientific computing. Python is easy to learn, has a very clear syntax and can easily be extended with modules written in C, C++ or FORTRAN.
# 
# The Biopython web site (http://www.biopython.org) provides an online resource for modules, scripts, and web links for developers of Python-based software for bioinformatics use and research. Basically, the goal of Biopython is to make it as easy as possible to use Python for bioinformatics by creating high-quality, reusable modules and classes. Biopython features include parsers for various Bioinformatics file formats (BLAST, Clustalw, FASTA, Genbank,...), access to online services (NCBI, Expasy,...), interfaces to common and not-so-common programs (Clustalw, DSSP, MSMS...), a standard sequence class, various clustering modules, a KD tree data structure etc. and even documentation.
# 
# Basically, we just like to program in Python and want to make it as easy as possible to use Python for bioinformatics by creating high-quality, reusable modules and scripts.
# 

# ## What can I find in the Biopython package
# 

# The main Biopython releases have lots of functionality, including:
# 
# - The ability to parse bioinformatics files into Python utilizable data structures, including support for the following formats:
#   - Blast output – both from standalone and WWW Blast
#   - Clustalw
#   - FASTA
#   - GenBank
#   - PubMed and Medline
#   - ExPASy files, like Enzyme and Prosite
#   - SCOP, including ‘dom’ and ‘lin’ files
#   - UniGene
#   - SwissProt
# - Files in the supported formats can be iterated over record by record or indexed and accessed via a Dictionary interface.
# - Code to deal with popular on-line bioinformatics destinations such as:
#   - NCBI – Blast, Entrez and PubMed services
#   - ExPASy – Swiss-Prot and Prosite entries, as well as Prosite searches
# - Interfaces to common bioinformatics programs such as:
#   - Standalone Blast from NCBI
#   - Clustalw alignment program
#   - EMBOSS command line tools
# -A standard sequence class that deals with sequences, ids on sequences, and sequence features.
# - Tools for performing common operations on sequences, such as translation, transcription and weight calculations.
# - Code to perform classification of data using k Nearest Neighbors, Naive Bayes or Support Vector Machines.
# - Code for dealing with alignments, including a standard way to create and deal with substitution matrices.
# - Code making it easy to split up parallelizable tasks into separate processes.
# - GUI-based programs to do basic sequence manipulations, translations, BLASTing, etc.
# - Extensive documentation and help with using the modules, including this file, on-line wiki documentation, the web site, and the mailing list.
# - Integration with BioSQL, a sequence database schema also supported by the BioPerl and BioJava projects.
# 
# We hope this gives you plenty of reasons to download and start using Biopython!
# 

# ## About these notebooks
# 

# These notebooks were prepared on Python 3 for Project Jupyter 4+ (formely IPython Notebook). Biopython should be installed and available (v1.66 or newer recommended).
# 
# You can check the basic installation and inspect the version by doing:
# 

import Bio
print(Bio.__version__)





# **Source of the materials**: Biopython cookbook (adapted)
# <font color='red'>Status: Draft</font>
# 

# Appendix: Useful stuff about Python {#sec:appendix}
# ===================================
# 
# If you haven’t spent a lot of time programming in Python, many questions
# and problems that come up in using Biopython are often related to Python
# itself. This section tries to present some ideas and code that come up
# often (at least for us!) while using the Biopython libraries. If you
# have any suggestions for useful pointers that could go here, please
# contribute!
# 
# What the heck is a handle? {#sec:appendix-handles}
# --------------------------
# 
# Handles are mentioned quite frequently throughout this documentation,
# and are also fairly confusing (at least to me!). Basically, you can
# think of a handle as being a “wrapper” around text information.
# 
# Handles provide (at least) two benefits over plain text information:
# 
# 1.  They provide a standard way to deal with information stored in
#     different ways. The text information can be in a file, or in a
#     string stored in memory, or the output from a command line program,
#     or at some remote website, but the handle provides a common way of
#     dealing with information in all of these formats.
# 
# 2.  They allow text information to be read incrementally, instead of all
#     at once. This is really important when you are dealing with huge
#     text files which would use up all of your memory if you had to load
#     them all.
# 
# Handles can deal with text information that is being read (e. g. reading
# from a file) or written (e. g. writing information to a file). In the
# case of a “read” handle, commonly used functions are `read()`, which
# reads the entire text information from the handle, and `readline()`,
# which reads information one line at a time. For “write” handles, the
# function `write()` is regularly used.
# 
# The most common usage for handles is reading information from a file,
# which is done using the built-in Python function `open`. Here, we open a
# handle to the file [m\_cold.fasta](data/m_cold.fasta) (also
# available online
# [here](http://biopython.org/DIST/docs/tutorial/examples/m_cold.fasta)):
# 
# 

handle = open("data/m_cold.fasta", "r")
handle.readline()


# 
# Handles are regularly used in Biopython for passing information to
# parsers. For example, since Biopython 1.54 the main functions in
# `Bio.SeqIO` and `Bio.AlignIO` have allowed you to use a filename instead
# of a handle:
# 
# 

from Bio import SeqIO
for record in SeqIO.parse("data/m_cold.fasta", "fasta"):
    print(record.id, len(record))


# 
# On older versions of Biopython you had to use a handle, e.g.
# 
# 

from Bio import SeqIO
handle = open("data/m_cold.fasta", "r")
for record in SeqIO.parse(handle, "fasta"):
    print(record.id, len(record))
handle.close()


# 
# This pattern is still useful - for example suppose you have a gzip
# compressed FASTA file you want to parse:
# 
# 

# ```python
# import gzip
# from Bio import SeqIO
# handle = gzip.open("m_cold.fasta.gz")
# for record in SeqIO.parse(handle, "fasta"):
#     print(record.id, len(record))
# handle.close()
# ```
# 

# 
# See Section \[sec:SeqIO\_compressed\] for more examples like this,
# including reading bzip2 compressed files.
# 
# ### Creating a handle from a string
# 
# One useful thing is to be able to turn information contained in a string
# into a handle. The following example shows how to do this using
# `cStringIO` from the Python standard library:
# 
# 

my_info = 'A string\n with multiple lines.'
print(my_info)


from io import StringIO
my_info_handle = StringIO(my_info)
first_line = my_info_handle.readline()
print(first_line)


second_line = my_info_handle.readline()
print(second_line)





# **Source of the materials**: Biopython cookbook (adapted)
# <font color='red'>Status: Draft</font>
# 

# <img src="images/biopython.jpg">
# 

# # Biopython
# Tutorial and Cookbook
# --------------------
# [1 - Introduction](01 - Introduction.ipynb)
# 
# [2 - Quick Start](02 - Quick Start.ipynb)
# 
# [3 - Sequence Objects](03 - Sequence Objects.ipynb)
# 
# [4 - Sequence Annotation objects](04 - Sequence Annotation objects.ipynb)
# 
# [5 - Sequence Input and Output](05 - Sequence Input and Output.ipynb)
# 
# [6 - Multiple Sequence Alignment objects](06 - Multiple Sequence Alignment objects.ipynb)
# 
# [7 - Blast](07 - Blast.ipynb)
# 
# [8 - BLAST and other sequence search tools (experimental code)](08 - BLAST and other sequence search tools - experimental code.ipynb)
# 
# [9 - Accessing NCBI’s Entrez databases](09 - Accessing NCBIs Entrez databases.ipynb)
# 
# [10 - Swiss-Prot and ExPASy](10 - Swiss-Prot and ExPASy.ipynb)
# 
# [11  Going 3D - The PDB module](11  Going 3D - The PDB module.ipynb)
# 
# [12 - Bio.PopGen - Population Genetics](12 - Bio.PopGen - Population Genetics.ipynb)
# 
# [13 - Phylogenetics with Bio.Phylo](13 - Phylogenetics with Bio.Phylo.ipynb)
# 
# [14 - Sequence motif analysis using Bio.motifs](14 - Sequence motif analysis using Bio.motifs.ipynb)
# 
# [15 - Cluster Analysis](15 - Cluster Analysis.ipynb)
# 
# [16 - Supervised learning methods](16 - Supervised learning methods.ipynb)
# 
# [17 - Graphics including GenomeDiagram](17 - Graphics including GenomeDiagram.ipynb)
# 
# [18 - KEGG](18 - KEGG.ipynb)
# 
# [19 - Cookbook – Cool things to do with it](19 - Cookbook – Cool things to do with it.ipynb)
# 
# [20 - The Biopython testing](20 - The Biopython testing.ipynb)
# 
# [21 - Advanced](21 - Advanced.ipynb)
# 
# [22 - Where to go from here – contributing to Biopython](22 - Where to go from here – contributing to Biopython.ipynb)
# 
# [23 - Appendix, Useful stuff about Python](23 - Appendix, Useful stuff about Python.ipynb)
# 
# [Credits](99 - Credits.ipynb)
# 
# [References](99 - References.ipynb)
# 




# <img src="notebooks/images/biopython.jpg">
# 

# # Welcome to Biopython's web tutorial
# 
# [Start Here!](notebooks/00 - Tutorial - Index.ipynb)
# 




